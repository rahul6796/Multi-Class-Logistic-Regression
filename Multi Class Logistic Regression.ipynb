{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Class Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1$. What is Multi-class-Logistic-Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$.Objective-Function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3$.Loss-Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$.Optimization-Of-Loss-Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is Multi-class-Logistic-Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ${Multi-Class-Logistic-Regression}$ we are try to classify the more that two class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and there intution is come from the ${Naive-Bayes-For-Multi-Classification}$. and in this case all things are different from the Binary class logistic regression, like ${Objective-Function}$ and ${loss-Function}$ etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we come up with objective function for ${Multi-class-logistic-regression}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Objective-Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the formula of Naive-Bayes-For-Multi-class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have only ${3}$ ${class}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${Bayes Rule}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(B|A)\\cdot{P(A)}}{P(B|A)\\cdot{P(A)} + P(B|A^\\complement)\\cdot{P(A^\\complement)}} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so Naive-Bayes-for-three-class is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In given Data our Feature are represented as X which containing N examples of our training-data look's like. ${[{x}_1,{x}_2,{x}_3,{x}_4.................{x}_N]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and our target variabels which can represented as c which containing three class       ${[0,1,2]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c=0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(c=0|X) = \\frac{P(X|c={0})\\cdot{P(c={0})}}{P(X|c={0})\\cdot{P(c={0})} +P(X|c={1})\\cdot{P(c={1})} +P(X|c={2})\\cdot{P(c={2})}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c=1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(c=1|X) = \\frac{P(X|c={1})\\cdot{P(c={1})}}{P(X|c={0})\\cdot{P(c={0})} +P(X|c={1})\\cdot{P(c={1})} +P(X|c={2})\\cdot{P(c={2})}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c=2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(c=2|X) = \\frac{P(X|c={2})\\cdot{P(c={2})}}{P(X|c={0})\\cdot{P(c={0})} +P(X|c={1})\\cdot{P(c={1})} +P(X|c={2})\\cdot{P(c={2})}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In given data, our data is balanced so ${Prior-Probability }$ for each class are same. so our updated Naive-Bayes-formula for each classes is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c=0}$.\n",
    "\\begin{equation}\n",
    "P(c=0|X) = \\frac{P(X|c={0})}{P(X|c={0}) +P(X|c={1}) +P(X|c={2})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c=1}$.\n",
    "\\begin{equation}\n",
    "P(c=1|X) = \\frac{P(X|c={1})}{P(X|c={0}) +P(X|c={1}) +P(X|c={2})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c=2}$.\n",
    "\\begin{equation}\n",
    "P(c=2|X) = \\frac{P(X|c={2})}{P(X|c={0}) +P(X|c={1}) +P(X|c={2})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we are introducing some thing intreseting. we can used some log properties like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "x=e^{\\log_{e}{x}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we taking a single class for desing a general formula for multi-class-logistic-regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive-bayes-for-multiclass is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(c=1|X) = \\frac{e^{\\log_{e}{P(X|c={1})}}}{e^{\\log_{e}{P(X|c={0})}}+e^{\\log_{e}{P(X|c={1})}}+e^{\\log_{e}{P(X|c={2})}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our random-variables is ${Multivariate-Gaussian-Distribution}$ so here ${Likelihood-Probability}$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(x;{\\mu},{\\Sigma}) = \\frac{1}{\\sqrt{2{\\pi}}\\sqrt{\\Sigma}}e^{(-\\frac{1}{2}(x-{\\mu})^T{\\Sigma}^{-1}(x-{\\mu}))}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we write ${X\\sim{\\mathcal{N}}({\\mu,{\\Sigma}})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that our ${\\Sigma}$ is ${Non-singular}$ matrix. so we can say our ${\\hat{\\Sigma_{0}} =\\hat{\\Sigma_{1}}=\\hat{\\Sigma_{2}} ={\\Sigma_{p}}}$ which is equal to ${Pooled-Covariance-Matrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we take single class and explain the concept of the ${Likelihood-Probability}$ in case of ${Multivariate-Gaussian-Distribution}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(X|c={1}) = \\frac{1}{\\sqrt{2{\\pi}}\\sqrt{\\Sigma_{p}}}e^{(-\\frac{1}{2}(X-{\\mu_{0}})^T{\\Sigma_{p}}^{-1}(X-{\\mu_{0}}))}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taking log both side then we get a kind of ${Discriminant-Function}$. having some parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "{\\log_{e}P(X|c=1)} = \\hat{\\theta}_{0}^{1} + \\hat{{\\theta}^{1}}^T\\cdot{X}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same for rest of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "{\\log_{e}P(X|c=0)} = \\hat{\\theta}_{0}^{0} + \\hat{{\\theta}^{0}}^T\\cdot{X}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "{\\log_{e}P(X|c=2)} = \\hat{\\theta}_{0}^{2} + \\hat{{\\theta}^{2}}^T\\cdot{X}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above ${Discriminat-Function}$ the parameters are different for diferent classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our classifier become."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c=1}$.\n",
    "\\begin{equation}\n",
    "P(c={1}|X) = \\frac{e^{(\\hat{\\theta}_{0}^{1} + \\hat{{\\theta}^{1}}^T\\cdot{X})}}{e^{(\\hat{\\theta}_{0}^{0} + \\hat{{\\theta}^{0}}^T\\cdot{X})} +e^{(\\hat{\\theta}_{0}^{1} + \\hat{{\\theta}^{1}}^T\\cdot{X})} + e^{(\\hat{\\theta}_{0}^{2} + \\hat{{\\theta}^{2}}^T\\cdot{X})}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c=0}$.\n",
    "\\begin{equation}\n",
    "P(c={0}|X) = \\frac{e^{(\\hat{\\theta}_{0}^{0} + \\hat{{\\theta}^{0}}^T\\cdot{X})}}{e^{(\\hat{\\theta}_{0}^{0} + \\hat{{\\theta}^{0}}^T\\cdot{X})} +e^{(\\hat{\\theta}_{0}^{1} + \\hat{{\\theta}^{1}}^T\\cdot{X})} + e^{(\\hat{\\theta}_{0}^{2} + \\hat{{\\theta}^{2}}^T\\cdot{X})}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c=2}$.\n",
    "\\begin{equation}\n",
    "P(c={2}|X) = \\frac{e^{(\\hat{\\theta}_{0}^{2} + \\hat{{\\theta}^{2}}^T\\cdot{X})}}{e^{(\\hat{\\theta}_{0}^{0} + \\hat{{\\theta}^{0}}^T\\cdot{X})} +e^{(\\hat{\\theta}_{0}^{1} + \\hat{{\\theta}^{1}}^T\\cdot{X})} + e^{(\\hat{\\theta}_{0}^{2} + \\hat{{\\theta}^{2}}^T\\cdot{X})}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generalized form of our final classifier for ${Multi-class-logistic-regression}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c = 0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(c=0|X) = \\frac{e^{\\hat{\\theta}_{0}^{0} + \\hat{{\\theta}^{0}}^T\\cdot{X}}}{\\sum_{k=0}^2e^{(\\hat{\\theta}_{0}^{k} + \\hat{{\\theta}^{k}}^T\\cdot{X})}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c=1}$.\n",
    "\\begin{equation}\n",
    "P(c=1|X) = \\frac{e^{\\hat{\\theta}_{0}^{1} + \\hat{{\\theta}^{1}}^T\\cdot{X}}}{\\sum_{k=0}^2e^{(\\hat{\\theta}_{0}^{k} + \\hat{{\\theta}^{k}}^T\\cdot{X})}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for class ${c=2}$.\n",
    "\\begin{equation}\n",
    "P(c=2|X) = \\frac{e^{\\hat{\\theta}_{0}^{2} + \\hat{{\\theta}^{2}}^T\\cdot{X}}}{\\sum_{k=0}^2e^{(\\hat{\\theta}_{0}^{k} + \\hat{{\\theta}^{k}}^T\\cdot{X})}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above classifier for each class is called ${Softmax-Function}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our ${Objective-Function}$ in case of ${Multi-class-logistic-regression}$ is ${Softmax-Function}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's find out Loss function for ${Multi-class-logistic-regression}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is minimized our bojective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 .Loss-Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that Likelihood function for Bernoulli Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(y|x) = \\hat{y}^{y}\\cdot{\\hat(1-\\hat{y})^{(1-y)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here ${\\hat{y}}$ is best parameters of  this distribution and you can say also this the predicted answer when your ${y=1}$ is ${\\hat{y}}$ and for ${y=0}$ is ${1-\\hat{y}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but in our case there is three class so how we can define this likelihood probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes is ${[0,1,2]}$ and  and there corresponding best estimates are ${\\hat{p}}$ for ${0}$ class ,${\\hat{q}}$ for ${1}$ class and ${\\hat{r}}$ is for ${2}$ class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you write the posterior probability for these three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(c=0|X) = (\\hat{p})^{1}\\cdot{(\\hat{q}})^{0}\\cdot({\\hat{r}})^{0}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(c=1|X) = (\\hat{p})^{0}\\cdot{(\\hat{q}})^{1}\\cdot({\\hat{r}})^{0}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(c=2|X) = (\\hat{p})^{0}\\cdot{(\\hat{q}})^{0}\\cdot({\\hat{r}})^{1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so if you write in combined formed the this is be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P({c}^{i}=k|X)  = (P(c^{i}={0})|X)^{c_{0}^{i}}\\cdot{(P(c^{i}={1})|X)^{c_{1}^{i}}}\\cdot{(P(c^{i}={2})|X)^{c_{2}^{i}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(c^{i}=k|X) = \\prod_{k=0}^{2}P(c^{i}=k|X)^{c_{k}^{i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${k}$ = ${[0,1,2]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In given dataset our features are IID assumption which is called (Independent and Indentically Distributed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so our ${Likelihood-Function}$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L = \\prod_{i=1}^NP(c^{i}=k|X)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L = \\prod_{i=1}^N\\prod_{k=0}^{2}[P(c^{i}=k|X)]^{c_{k}^{i}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we take log_{e} both the side then our ${likelihood-function}$ become ${log -likelihood-function}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "{\\log_e L} ={\\log_e} (\\prod_{i=1}^N\\prod_{k=0}^{2}[P(c^{i}=k|X)]^{c_{k}^{i}})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
